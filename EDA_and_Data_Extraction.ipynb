{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMGrMEniZesbiDPd+eRlk2w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SteinwayThor/eeg-features-movies/blob/main/EDA_and_Data_Extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generic torch imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.init as init\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchsummary import summary\n",
        "\n",
        "# generic ml/stats imports\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "import numpy as np\n",
        "import scipy.fftpack as sp\n",
        "from sklearn.model_selection import train_test_split\n",
        "from math import pi\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "\n",
        "#torch vision related imports\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from torchvision import datasets\n",
        "\n",
        "# data utils\n",
        "import time\n",
        "import os\n",
        "import pickle\n",
        "import json\n",
        "from PIL import Image\n",
        "from google.colab import drive\n",
        "import bz2\n",
        "import _pickle as cPickle\n",
        "import pickle as pl"
      ],
      "metadata": {
        "id": "JbwgQF4Ad6k4"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "h34_HdbfdgHY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61e964c3-cb8e-4819-dbdb-f6f9c07adfcf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Load any compressed pickle file\n",
        "def decompress_pickle(file):\n",
        "  data = bz2.BZ2File(file, 'rb')\n",
        "  data = cPickle.load(data)\n",
        "  return data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emotion_annotations_f = '/content/drive/MyDrive/Movie Data/e0018RI/emotion_annotations'  #might need to add Movie Data back in path\n",
        "emotion_annotations = os.listdir(emotion_annotations_f)"
      ],
      "metadata": {
        "id": "cr-FZ3Y7jehd"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#EDA on emotions json data\n",
        "diff_emotions = {}\n",
        "diff_viewers = {}\n",
        "diff_chapters = {}\n",
        "for json_file in emotion_annotations:\n",
        "  split_name = json_file.split(\"_\")\n",
        "  emotion = split_name[2]  #angry\n",
        "  viewer = split_name[3]  #viewer127\n",
        "  chapter = split_name[4][:split_name[4].find(\".\")]  #chapter01\n",
        "  diff_emotions[emotion] = diff_emotions.get(emotion,[]).append(json_file)\n",
        "  diff_viewers[viewer] = diff_viewers.get(viewer,[]).append(json_file)\n",
        "  diff_chapters[chapter] = diff_chapters.get(chapter,[]).append(json_file)\n",
        "print(diff_chapters.keys(), diff_emotions.keys(), diff_viewers.keys())"
      ],
      "metadata": {
        "id": "pmyS-9E6l_uL",
        "outputId": "c3ea1a74-4b89-454a-a8b4-5320859bdcc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-a38006a258c9>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mviewer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_name\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m#viewer127\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mchapter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_name\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msplit_name\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m#chapter01\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m   \u001b[0mdiff_emotions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0memotion\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiff_emotions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memotion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m   \u001b[0mdiff_viewers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiff_viewers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mdiff_chapters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchapter\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiff_chapters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchapter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'append'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f = open(os.path.join(emotion_annotations_f, emotion_annotations[0]))\n",
        "data = json.load(f)\n",
        "print(emotion_annotations[0])\n",
        "\n",
        "json_formatted_str = json.dumps(data, indent=2)\n",
        "print(json_formatted_str)\n"
      ],
      "metadata": {
        "id": "ICODGzYuj7fC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data =\n",
        "# Create an empty array\n",
        "#print(data[\"tracks\"])\n",
        "label_frames = []\n",
        "for track in data[\"tracks\"]:\n",
        "    shapes = track[\"shapes\"]\n",
        "    frame = shapes[0][\"frame\"]\n",
        "    attributes = shapes[0][\"attributes\"]\n",
        "    if attributes != []:\n",
        "        emotion = attributes[0][\"value\"]\n",
        "        label_frames.append((emotion, frame))\n",
        "\n",
        "def find_last_frame(data):\n",
        "    last_frame = 0\n",
        "    for track in data[\"tracks\"]:\n",
        "        shapes = track[\"shapes\"][-1]\n",
        "        frame = shapes[\"frame\"]\n",
        "        last_frame = frame\n",
        "    return last_frame\n",
        "\n",
        "def create_tuple_array(data):\n",
        "    emotions_frames = []\n",
        "    current_emotion = \"not_angry\" #get last emotion from previous dataset\n",
        "    for i in range(find_last_frame(data)):\n",
        "        if i in label_frames:\n",
        "            current_emotion = label_frames[0]\n",
        "        pair = (current_emotion, i)\n",
        "        emotions_frames.append(pair)\n",
        "    return emotions_frames\n",
        "print(len(create_tuple_array(data)))"
      ],
      "metadata": {
        "id": "_X7aIF0SkpT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load in neural data\n",
        "# NOTE: Takes a while!\n",
        "neural_data_folder = '/content/drive/MyDrive/Movie Data/e0018RI/2023-04-02_e0018RI_00'\n",
        "\n",
        "neural_data_dir = os.listdir(neural_data_folder)\n",
        "\n",
        "neural_data = {}\n",
        "channels = []\n",
        "for f in neural_data_dir:\n",
        "  channel_name = f[f.rfind('_')+1:-5]\n",
        "  channels.append(channel_name)\n",
        "  # channel_data = decompress_pickle(os.path.join(neural_data_folder, f))\n",
        "  # neural_data[channel_name] = channel_data\n",
        "\n",
        "print(channels)"
      ],
      "metadata": {
        "id": "BIYn3FT6krY4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}